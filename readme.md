# Project Description

This repo provides code for evaluating the robustness of data-incremental learning against a variety of input and
label corruptions. While some works have shown that continual learning is more susceptible to label noise,
the mechanisms explaining how this happens are not well-understood, and this repo aims to bridge that gap by 

* Considering noise that is conditional on sample difficulty to reveal disparate subgroup effects
* Showing how features and decision boundaries evolve over time
* Decoupling the risk of forgetting clean samples from that of memorizing noisy samples to see if replay buffers help or hurt performance in a noisy data setting

# Repo Structure

The structure of this repo is 
* `configs` (has all the .yaml files which configure model training)
* `src` (our code files which define models, datasets, imputers, etc.)

In order to run the code another directory called `data` needs to be created, which houses the datasets used.

## src

`src/scripts` contains the scripts which train models. Experiments are configured using [hydra](https://hydra.cc/docs/intro/) which enables the use of
the factory design pattern when creating different models, datasets, etc. 

### Data
The datasets in `src/data` are encapsulated by a custom `DataModule`
which extends the PyTorch Lightning DataModule class and implements additional methods and properties relevant for continual
learning. In particular, each `DataModule` is composed of a `Splitter` which is responsible for splitting the data into train/update/test partitions, and a `Feeder` which determines how to slice the underlying dataset
into multiple parts for each update and whether or not to include the original training partition when making an update.

### Models
Every model in `src/models` extends the `Model` class which is a subclass of `torch.nn.Module` and provides additional
methods for predicting probability (`predict_proba()`) and making thresholded predictions (`predict()`).  

Models are wrapped in a `Module` defined in `src/modules`. Each `Module` is a subclass of the of the PyTorch Lightning
`LightningModule` and provides convenience methods for configuring optimizers, defining forward passes, computing losses/metrics, etc.

### Input Corruptors
Input corruptors (`src/input_corruptors`) are used to corrupt input features, e.g. image pixels. Each corruptor is a subclass of the `InputCorruptor` abstract class
and must implement methods for determining which samples are eligible for corruption and the samples which actually end up
being corrupted such as `get_potential_indices()` and `get_relevant_indices()`. Every input corruptor has an `Applicator` which
actually performs a given corruption such as blurring or pixelating an image. Decoupling the sample selection behaviour
from the input noise itself provides a useful degree of flexibility.

### Label Corruptors
Label corruptors (`src/label_corruptors`) are used to corrupt labels, and each corruptor is a subclass of the `LabelCorruptor` abstract class. Each 
corruptor must implement methods for determining which samples are eligible for corruption and the samples which actually end up
being corrupted such as `get_potential_indices()` and `get_relevant_indices()`. Label noise is generated by randomly choosing
a new label for the sample that is to be corrupted. 

### Inferers
Evaluating model performance requires performing inference, and the inferers in `src/inferers` implement this functionality.
Every inferer must implement the `make_predictions()` method defined in the `Inferer` abstract class which subclasses
the PyTorch Lightning Trainer. There are inferers for extracting predictions, embeddings, and even input images themselves,
all of which avoid the use of for loops.

### Trackers
Model performance,  predictions, and the samples which were corrupted are tracked using the trackers in `src/trackers`. Each tracker
implements the `TrackerInterface` which defines the skeleton for methods to track predictions, and get the final list/dataframe
of predictions once all model updates have been performed. 

### Other Packages

The remaining packages are briefly described below:

* `src/callbacks` defines callbacks for things like custom early stopping, checkpointing, and tracking predictions throughout training
* `src/lr_schedulers` wraps existing PyTorch learning rate schedulers so that creating them has a standardized interface
* `src/optimizers` wraps existing PyTorch optimizers so that creating them has a standardized interface
* `src/threshold_selectors` has methods for choosing prediction thresholds useful for binary classifiers
* `src/trainers` has subclassed PyTorch Lightning trainers
* `src/utils` contains utility functions for computing metrics, similarity between embeddings, and loading models/checkpoints

# Extending the Code
Since the repo relies heavily on the factory design method, adding new models, datasets, corruptors, etc. is simple. 
For example, creating a new model requires only adding a new python file defining the model class in `src/models`, subclassing
the `Model` class, and implementing the necessary abstract methods. Then, the `models` factory needs to be imported from
`src/models/creation.py` in order to register the builder for this new model via `models.register_builder("new_model", NewModel)`.
An import statement needs to be added to `src/models/\_\_init\_\_.py` as well to import the `NewModel` class.
Lastly, a yaml configuration file for the new model should be added to `configs/model` following the structure of configs
for existing models. Once this is done, the model can easily be created in the code via

```python
model = models.create(args.model.name, data_dimension=data_module.data_dimension,
                      num_classes=data_module.num_classes, **args.model.params)
```

This avoids the need for unwieldy if/else blocks, and allows for models that take in arbitrary parameters to be created
in a standardized way. For example, `src/models/mlp.py` defines a fully connected architecture with a flexible number of layers
and activation function depending on the `hidden_layers` and `activation` arguments. `src/models/resnet18.py` simply wraps the 
Torchvision implementation of ResNet, and does not use the arguments that `src/models/mlp.py` does. Nevertheless, both models
can be created in the same standardized way via the factory approach.

# Libraries

To install the necessary libraries, run the command

`pip install -r requirements.txt`

# Model Training

Much of the training code depends on [wandb](https://wandb.ai/site), so in order to run any experiments, one has to locally setup their 
wandb account such that they can login with their API key by entering their key in `src/utils/wandb.py`. Additionally, 
 users should have a project named `final_project` created in their wandb account. 

Add the path to the main directory (one level above `src`) to the `PYTHONPATH` via

`export PYTHONPATH="${PYTHONPATH}:$(pwd)"`

To train a model, one needs to call 

`python src/scripts/run_update_pipeline.py`

which will use the default arguments from `configs/update_pipeline.yaml`. To specify custom arguments,
i.e. change the dataset or model, these parameters can be specfied at the command line via

`python src/scripts/run_update_pipeline.py model=resnet18 data=cifar10_multiclass`

Some arguments are trickier to specify using hydra's syntax, so changing the number of updates to 10 can be done via

`python src/scripts/run_update_pipeline.py data_module.params.feeder_args.params.num_updates=10`